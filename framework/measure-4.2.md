[//]: # (COPYRIGHT)
[//]: # (RiskFrame.ai - AI Risk Management and Resilience Framework)
[//]: # (Copyright (C) 2024 RiskFrame.ai https://www.riskframe.ai https://github.com/riskframe/ai-rmm)
[//]: # (SOFTWARE LICENSE)
[//]: # (This file is part of AI-RMM, which is distributed under GNU General Public License V3. See LICENSE.txt to get a full copy.)
    
## Measure 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI actors to validate whether the system is performing consistently as intended. Results are documented.

### Measure 4.2.1. Gather Trustworthiness Measurement Data.

#### Sub Practices

1. Continuously collect and analyze data related to the AI system's trustworthiness, including metrics such as explainability, fairness, validity, and value.

2. Utilize various data sources, such as system logs, user feedback, and performance assessments, to capture a comprehensive picture of the AI system's trustworthiness.

3. Employ advanced data analytics techniques to identify patterns and insights from the collected data.

### Measure 4.2.2. Involve Domain Experts and Relevant AI Actors.

#### Sub Practices

1. Engage domain experts and relevant AI actors, such as AI developers, operators, and decision-makers, in the interpretation of trustworthiness measurement results.

2. Collaborate with domain experts to understand the context and implications of the data, ensuring that interpretations are grounded in domain-specific knowledge.

3. Solicit input from AI actors to identify potential biases or limitations in the data and interpretation methods.

### Measure 4.2.3. Validate System Performance against Intended Design.

#### Sub Practices

1. Compare trustworthiness measurement results against the AI system's intended design and performance specifications.

2. Identify discrepancies between the actual performance and the intended design, considering factors such as data quality, algorithm robustness, and human-machine interactions.

3. Analyze the root causes of these discrepancies to determine whether they are due to system errors, operational issues, or underlying design flaws.

### Measure 4.2.4. Document Trustworthiness Measurement Results.

#### Sub Practices

1. Maintain a comprehensive record of trustworthiness measurement results, including data sources, analysis methodologies, and interpretations.

2. Document insights gained from domain expert and AI actor involvement, ensuring transparency and accountability in trustworthiness assessment.

3. Share trustworthiness measurement results with relevant stakeholders to promote informed decision-making and continuous improvement.

### Measure 4.2.5. Continuously Monitor and Adapt Trustworthiness Evaluation.

#### Sub Practices

1. Regularly monitor trustworthiness measurement results to identify emerging trends, potential risks, and opportunities for improvement.

2. Adapt trustworthiness evaluation methodologies and data sources as needed to address changing requirements, technological advancements, or evolving user needs.

3. Foster a culture of continuous learning and improvement in trustworthiness assessment throughout the AI lifecycle.

### Measure 4.2.6. Promote Trustworthiness-Driven AI Development.

#### Sub Practices

1. Integrate trustworthiness evaluation into the AI development lifecycle, ensuring that trustworthiness is considered at every stage of design, development, testing, deployment, and operation.

2. Encourage a mindset of proactive trustworthiness assessment, prioritizing preventive measures over reactive mitigation.

3. Make trustworthiness a key factor in evaluating the success and adoption of AI systems.

