[//]: # (COPYRIGHT)
[//]: # (RiskFrame.ai - AI Risk Management and Resilience Framework)
[//]: # (Copyright (C) 2024 RiskFrame.ai https://www.riskframe.ai https://github.com/riskframe/ai-rmm)
[//]: # (SOFTWARE LICENSE)
[//]: # (This file is part of AI-RMM, which is distributed under GNU General Public License V3. See LICENSE.txt to get a full copy.)
    
## Measure 2.3
> AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented. [@playbook]

### Measure 2.3.1. Establish Performance or Assurance Criteria.

To set the foundation for evaluating AI systems in environments akin to their intended use, it's crucial to define clear and specific performance or assurance criteria. These benchmarks should not only reflect the system's intended functionality but also encompass its ability to maintain reliability, safety, and ethical standards under various conditions that mirror real-world settings. Documenting these criteria provides a transparent and structured framework for ongoing assessment and validation, ensuring the AI system's trustworthiness throughout its lifecycle.

Setting benchmarks for AI systems entails defining clear, measurable criteria that resonate with the system's purpose and trustworthiness, while considering attributes like accuracy and fairness. Documenting the rationale for these choices ensures they are relevant and achievable, fostering a transparent framework for evaluation.

#### Sub Practices

1. Define clear and measurable performance or assurance criteria that align with the AI system's intended purpose, its trustworthiness characteristics, and the organization's risk tolerance.

2. Consider factors such as accuracy, fairness, robustness, explainability, and security when establishing criteria.

3. Document the rationale behind the selection of criteria, ensuring they are relevant, quantifiable, and achievable.

### Measure 2.3.2. Identify Representative Deployment Settings.

Pinpointing environments that closely mirror where the AI system will be deployed is a critical step in ensuring its effectiveness and trustworthiness. This involves a thorough analysis of potential use cases and operational conditions to simulate real-world challenges and opportunities the system may face. By identifying these representative settings, developers can tailor their evaluation strategies, ensuring that performance and assurance criteria are tested under conditions that accurately reflect the system's ultimate deployment context, thereby enhancing its reliability and applicability in practical scenarios.

Identifying the right contexts for AI deployment plays a key role in its success. This involves a comprehensive understanding of where the system will operate, factoring in user demographics, data access, and environmental conditions. Choosing settings that exemplify a spectrum of possible use cases and conditions is crucial for thorough testing. Furthermore, it's important to clearly justify these selections, linking them directly to the system's intended purpose and the organization's approach to risk, thereby ensuring relevance and alignment with broader objectives.

#### Sub Practices

1. Identify and characterize the deployment settings where the AI system will be used, considering factors such as user demographics, data availability, and operating environments.

2. Select representative deployment settings for performance or assurance testing, ensuring they cover a range of potential conditions and usage scenarios.

3. Document the rationale for selecting representative settings, ensuring they are relevant to the AI system's intended use and the organization's risk profile.

### Measure 2.3.3. Develop Measurement Protocols.

Crafting detailed protocols for measuring AI system performance and assurance is essential to validate its effectiveness in real-world scenarios. This process involves outlining specific methodologies, tools, and techniques to quantitatively and qualitatively assess the system against established criteria. Such protocols should be designed to replicate conditions akin to the intended deployment settings as closely as possible, ensuring that the evaluation reflects practical challenges and requirements. Documenting these protocols provides a clear, replicable roadmap for consistent assessment and aids in maintaining the system's trustworthiness throughout its lifecycle.

Establishing comprehensive protocols for evaluating AI systems is pivotal. This entails devising detailed guidelines for gathering, analyzing, and interpreting data to assess the system's performance and assurance against set benchmarks. It also involves specifying quantitative metrics that accurately reflect the system's behavior and are sensitive to any deviations. Moreover, qualitative methodologies need to be developed to assess critical trustworthiness aspects like fairness, explainability, and robustness, ensuring a holistic evaluation of the AI system's capabilities and reliability.

#### Sub Practices

1. Develop detailed measurement protocols that outline the procedures for collecting, analyzing, and interpreting data related to the AI system's performance or assurance criteria.

2. Define the metrics to be used for quantitative assessments, ensuring they are relevant, reliable, and sensitive to changes in the AI system's behavior.

3. Establish qualitative assessment methodologies for evaluating trustworthiness characteristics such as fairness, explainability, and robustness.

### Measure 2.3.4. Conduct Performance or Assurance Testing.

Executing rigorous testing on AI systems for performance and assurance is a crucial phase in validating their readiness for deployment. This involves applying the previously developed measurement protocols in scenarios that closely resemble the intended operational environments. The aim is to assess whether the AI system meets the established criteria under realistic conditions, thereby ensuring its effectiveness, reliability, and trustworthiness. Documentation of test results is vital, as it provides tangible evidence of the system's capabilities and areas for potential improvement.

Implementing performance or assurance testing in conditions that mimic real deployment scenarios is essential. This step requires adherence to predefined protocols, employing suitable technologies and tools for comprehensive evaluation. Gathering data from these tests, which include both quantitative metrics and qualitative insights, is crucial for gauging the AI system's adherence to its performance and assurance benchmarks. Thorough documentation of the results, particularly noting any variances from anticipated performance, is key to understanding the system's readiness and areas needing refinement.

#### Sub Practices

1. Conduct performance or assurance testing in representative deployment settings, following the established protocols and utilizing appropriate tools and technologies.

2. Collect data from testing runs, including metrics and qualitative observations, to assess the AI system's performance or assurance against the established criteria.

3. Document the testing results, including any discrepancies between observed performance and expected outcomes.

### Measure 2.3.5. Analyze and Interpret Test Results.

Careful examination and interpretation of test results are pivotal in determining the AI system's alignment with its intended performance and assurance benchmarks. This step goes beyond mere data collection, delving into the nuances of how the system's behavior matches up against the established criteria under simulated deployment conditions. Analysts must consider both quantitative metrics and qualitative observations to draw comprehensive conclusions about the system's efficacy, reliability, and trustworthiness. Such in-depth analysis not only highlights the system's current capabilities but also identifies areas for improvement, guiding further refinements to enhance its readiness for real-world application.

Delving into the test data to uncover patterns, trends, and areas needing enhancement is crucial for refining AI system performance and assurance. Interpreting these results within the context of the system's intended application, deployment environments, and the organization's risk tolerance is key to understanding its real-world viability. Drawing informed conclusions about the system's adherence to established criteria allows stakeholders to identify potential risks or shortcomings, ensuring a well-rounded evaluation process that supports continuous improvement.

#### Sub Practices

1. Analyze the collected data to identify patterns, trends, and potential areas for improvement in the AI system's performance or assurance.

2. Interpret the test results in the context of the AI system's intended use, deployment settings, and organizational risk profile.

3. Draw conclusions about the AI system's compliance with performance or assurance criteria and identify any potential risks or limitations.

### Measure 2.3.6. Document Performance or Assurance Demonstration.

Thorough documentation of the AI system's performance or assurance demonstration is essential to provide transparency and accountability throughout the evaluation process. This documentation should encompass detailed records of the testing methodologies employed, the data collected, and the resulting analysis and interpretations. By documenting each step of the demonstration process, including any discrepancies or challenges encountered, stakeholders gain insight into the system's capabilities and limitations. Additionally, clear documentation facilitates knowledge sharing and enables informed decision-making regarding the system's readiness for deployment in real-world settings.

Documenting the demonstration of performance or assurance involves compiling comprehensive records that capture all aspects of the evaluation process. This includes detailing the criteria, protocols, results, and analyses conducted. Screenshots, data visualizations, and explanatory narratives are essential for illustrating the system's performance in representative deployment settings effectively. Sharing this documentation with pertinent stakeholders fosters transparency and informed decision-making, enabling developers, testers, risk managers, and decision-makers to assess the system's readiness for real-world deployment thoroughly.

#### Sub Practices

1. Create comprehensive documentation that summarizes the performance or assurance criteria, measurement protocols, testing results, and analysis findings.

2. Document the demonstration of performance or assurance in representative deployment settings, including screenshots, data visualizations, and narrative explanations.

3. Share the documentation with relevant stakeholders, including developers, testers, risk managers, and decision-makers.

### Measure 2.3.7. Continuously Evaluate and Adapt Performance or Assurance Measures.

Continuously assessing and adapting performance or assurance measures is essential for maintaining the trustworthiness of AI systems over time. This ongoing evaluation ensures that the established criteria remain relevant and effective in capturing the system's performance in evolving deployment settings. By regularly reviewing and updating measurement protocols based on new insights, emerging challenges, and feedback from stakeholders, organizations can proactively address potential issues and optimize the system's reliability and effectiveness. This iterative process of evaluation and adaptation fosters a culture of continuous improvement, enhancing the AI system's ability to meet evolving expectations and uphold trustworthiness in its operation.

This practice involves regularly evaluating effectiveness, gathering stakeholder feedback, and maintaining a living document. This ongoing process identifies areas for improvement, incorporates new insights, and ensures alignment with organizational objectives, enabling the AI system to evolve effectively amidst emerging risks and changing requirements.

#### Sub Practices

1. Regularly evaluate the effectiveness of performance or assurance measures, identifying areas for improvement and adapting criteria as the AI system evolves and new risks emerge.

2. Gather feedback from stakeholders and incorporate new insights into the measurement protocols and testing procedures.

3. Maintain a living document that reflects the dynamic nature of the AI system's performance and assurance, ensuring it remains relevant and aligned with organizational objectives.

### Measure 2.3 Suggested Work Products

* Performance and Assurance Criteria Document - A comprehensive document detailing the clear and measurable criteria for evaluating the AI system's performance and assurance, including aspects such as accuracy, fairness, robustness, explainability, and security.
* Deployment Setting Analysis Report - A report that characterizes the intended deployment settings of the AI system, outlining factors such as user demographics, data availability, and environmental conditions, along with the rationale for their selection.
* Measurement Protocol Guidelines - A set of detailed guidelines that specify the procedures, tools, and techniques for collecting, analyzing, and interpreting data related to the AI system's performance or assurance criteria.
* Quantitative Metrics Definition Document - A document that defines the specific metrics to be used for quantitative assessments of the AI system, ensuring they are relevant, reliable, and sensitive to changes in behavior.
* Performance Testing Results Summary - A summary report of the performance or assurance testing conducted, including methodologies used, data collected, and preliminary findings.
* Test Result Analysis and Interpretation Report - A detailed report analyzing and interpreting the test results, identifying patterns, trends, and potential areas for improvement, and drawing conclusions about the AI system's compliance with established criteria.
* Performance or Assurance Demonstration Documentation - Comprehensive documentation that encapsulates the entire demonstration process, including the testing methodologies, data collected, analysis, and interpretations, along with any discrepancies or challenges encountered.
* Stakeholder Feedback and Adaptation Log - A log or document that captures stakeholder feedback on the performance or assurance measures and documents the adaptations made to the measurement protocols and criteria based on this feedback.
* Continuous Evaluation and Adaptation Plan - A dynamic document or plan that outlines the process for the regular evaluation and adaptation of performance or assurance measures, ensuring they remain effective and relevant over time.
