[//]: # (COPYRIGHT)
[//]: # (RiskFrame.ai - AI Risk Management and Resilience Framework)
[//]: # (Copyright (C) 2024 RiskFrame.ai https://www.riskframe.ai https://github.com/riskframe/ai-rmm)
[//]: # (SOFTWARE LICENSE)
[//]: # (This file is part of AI-RMM, which is distributed under GNU General Public License V3. See LICENSE.txt to get a full copy.)
    
# Measure 2: AI systems are evaluated for trustworthy characteristics.

## Measure 2.1: Test sets, metrics, and details about the tools used during Test & Evaluation, Validation & Verification (TEVV) are documented.

### Measure 2.1.1. Develop and Document Test Sets.

#### Sub Practices

1. Create comprehensive test sets that encompass a wide range of scenarios, inputs, and potential errors, covering the AI system's functionality, performance, and trustworthiness characteristics.

2. Ensure that test sets are tailored to the specific AI system and its application domain, considering potential biases, ethical implications, and potential impacts on affected communities.

3. Document the rationale behind the selection of test cases, justifications for excluding certain scenarios, and the overall test coverage strategy.

### Measure 2.1.2. Establish Clear Test Metrics.

#### Sub Practices

1. Define quantitative and qualitative metrics to assess the effectiveness of testing activities and evaluate the AI system's performance and trustworthiness.

2. Select metrics that are relevant, measurable, and aligned with the identified AI risks, trustworthiness characteristics, and organizational objectives.

3. Document the rationale behind the selection of metrics, ensuring they adequately capture the intended aspects of AI system evaluation.

### Measure 2.1.3. Identify and Document Testing Tools.

#### Sub Practices

1. Identify and select appropriate testing tools that can effectively automate test execution, analyze test results, and provide insights into the AI system's behavior and performance.

2. Evaluate the capabilities and limitations of testing tools, considering factors such as compatibility with the AI system's architecture, data format, and testing needs.

3. Document the selection criteria for testing tools, including their features, performance, and alignment with organizational standards.

### Measure 2.1.4. Establish Test Execution Procedures.

#### Sub Practices

1. Define detailed procedures for executing test sets, including data preparation, test case execution, and reporting of results.

2. Establish clear roles and responsibilities for test execution, ensuring that qualified individuals are responsible for conducting and documenting test results.

3. Document the test execution procedures, including the testing environment, configuration settings, and error handling mechanisms.

### Measure 2.1.5. Integrate Testing into Development Lifecycle.

#### Sub Practices

1. Incorporate testing activities into the AI system's development lifecycle, ensuring that testing is not an afterthought but an integral part of the development process.

2. Schedule regular testing cycles throughout the development process to identify and address potential issues early on, reducing the risk of introducing defects later in the lifecycle.

3. Integrate testing tools and procedures into the development environment, facilitating automated testing and continuous integration/continuous delivery (CI/CD) practices.

### Measure 2.1.6. Document Testing Results and Insights.

#### Sub Practices

1. Maintain a comprehensive record of all testing activities, including test cases executed, test results obtained, and any identified defects or issues.

2. Analyze testing results to identify patterns, trends, and potential areas for improvement.

3. Document the insights gained from testing, including recommendations for enhancing the AI system's performance, trustworthiness, and ethical compliance.

### Measure 2.1.7. Continuously Evaluate and Adapt Testing Approach.

#### Sub Practices

1. Regularly evaluate the effectiveness of the testing approach, test sets, and testing metrics to ensure they remain relevant and aligned with the evolving AI system's requirements and risk profile.

2. Gather feedback from AI developers, testers, and other stakeholders to identify areas for improvement and adapt the testing approach accordingly.

3. Keep testing tools up-to-date and incorporate new testing methodologies and technologies to maintain the effectiveness of testing activities.

