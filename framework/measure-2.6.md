[//]: # (COPYRIGHT)
[//]: # (RiskFrame.ai - AI Risk Management and Resilience Framework)
[//]: # (Copyright (C) 2024 RiskFrame.ai https://www.riskframe.ai https://github.com/riskframe/ai-rmm)
[//]: # (SOFTWARE LICENSE)
[//]: # (This file is part of AI-RMM, which is distributed under GNU General Public License V3. See LICENSE.txt to get a full copy.)
    
## Measure 2.6
> The AI system is evaluated regularly for safety risks â€“ as identified in the map function. The AI system to be deployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics reflect system reliability and robustness, real-time monitoring, and response times for AI system failures. [@playbook]

### Measure 2.6.1. Establish a Safety Risk Evaluation Framework.

Establishing a comprehensive safety risk evaluation framework is crucial for assessing and mitigating potential hazards associated with AI systems. This framework should encompass a systematic approach to identifying safety risks, assessing their severity and likelihood, and developing strategies to mitigate these risks effectively. By taking into account the potential impact of these risks on individuals, organizations, and society, the framework ensures that all safety concerns are thoroughly evaluated and addressed, maintaining the integrity and trustworthiness of the AI system.

Defining clear criteria for residual negative risk levels is essential within this framework. It helps determine the acceptability of remaining risks after mitigation efforts and whether additional measures are needed. This process ensures that the AI system's deployment does not exceed the organization's risk tolerance and that it can operate safely, even when pushed beyond its knowledge limits. By continuously monitoring and adjusting these criteria based on real-world performance and feedback, organizations can maintain a dynamic and responsive approach to managing safety risks in AI systems.

#### Sub Practices

1. Define a comprehensive framework for evaluating safety risks associated with the AI system, incorporating the identification, assessment, and mitigation of potential safety hazards.

2. Identify and categorize safety risks based on their severity, likelihood, and potential impact on individuals, organizations, or society.

3. Establish clear criteria for determining whether residual negative risk levels are acceptable or require further mitigation.

### Measure 2.6.2. Develop Safety Metrics and Thresholds.

Developing comprehensive safety metrics and thresholds is essential for monitoring and ensuring the AI system's safety and reliability. These metrics, both quantitative and qualitative, should accurately reflect the system's robustness, its ability to be monitored in real-time, and the responsiveness to potential failures. Establishing these metrics provides a clear, measurable way to assess the system's performance against safety standards, facilitating ongoing evaluations of its operational integrity and the effectiveness of safety protocols.

Defining precise thresholds for each safety metric is crucial in determining the AI system's compliance with organizational safety standards and risk tolerance levels. These thresholds act as benchmarks for acceptable performance, guiding decisions on whether the system's safety measures are sufficient or require enhancement. By continuously monitoring these metrics against defined thresholds, organizations can ensure that the AI system operates within safe parameters, maintaining a balance between functionality and safety, and can adapt to evolving safety requirements.

#### Sub Practices

1. Establish quantitative and qualitative safety metrics to assess the AI system's ability to operate safely and reliably.

2. Metrics should reflect system reliability and robustness, real-time monitoring capabilities, and response times for AI system failures.

3. Define thresholds for each safety metric to determine whether the AI system meets the organization's safety standards and risk tolerance.

### Measure 2.6.3. Conduct Regular Safety Risk Assessments.

Conducting regular safety risk assessments at every stage of the AI system's lifecycle is fundamental to maintaining its safety and reliability. These assessments, carried out during development, testing, and deployment, ensure that safety risks are continuously identified and addressed, keeping pace with changes in the system's functionality and operational environment. This proactive approach to safety management allows for the early detection of potential hazards, enabling timely interventions and minimizing the risk of safety incidents.

As the AI system evolves and its deployment context changes, it's crucial to remain vigilant for new and emerging safety risks. Regular reassessment of the system's safety profile and the effectiveness of existing mitigation strategies is necessary to ensure that safety measures remain effective. Adjustments to these measures may be required to counteract newly identified risks or to enhance the system's overall safety. This iterative process of assessment and adjustment ensures that the AI system can maintain high safety standards throughout its operational life, adapting to new challenges and maintaining alignment with organizational risk tolerance.

#### Sub Practices

1. Conduct regular safety risk assessments throughout the AI system's lifecycle, including during development, testing, and deployment phases.

2. Identify new or emerging safety risks as the AI system evolves and its environment changes.

3. Assess the effectiveness of mitigation strategies and adjust safety measures as needed.

### Measure 2.6.4. Prioritize Safety Risk Mitigation.

Prioritizing safety risks is essential to ensuring that the most critical safety issues within an AI system are addressed promptly and effectively. This involves assessing each identified risk based on its severity, likelihood of occurrence, and potential impact on the system and its users. By focusing on the most significant risks first, organizations can allocate resources more efficiently, ensuring that the measures implemented have the greatest possible effect on enhancing the system's overall safety.

Developing and implementing targeted mitigation strategies for these prioritized risks is the next crucial step. Each strategy should be tailored to effectively address the specific nature and context of the risk it targets. Following implementation, the effectiveness of these strategies must be continuously evaluated, with adjustments made as necessary to ensure their continued efficacy. This dynamic approach to risk mitigation allows for the agile adaptation of safety measures, ensuring that the AI system remains as safe as possible in the face of evolving challenges and operational environments.

#### Sub Practices

1. Prioritize safety risks based on their severity, likelihood, and potential impact, ensuring that the most critical safety issues are addressed first.

2. Develop and implement appropriate mitigation strategies to address identified safety risks.

3. Evaluate the effectiveness of mitigation strategies and make necessary adjustments.

### Measure 2.6.5. Implement Safe Fail Mechanisms.

Implementing safe fail mechanisms is a critical aspect of ensuring an AI system's safety, designed to manage unexpected or hazardous situations gracefully. These mechanisms are engineered to allow the system to degrade functionality or terminate operations safely, minimizing potential harm or disruption. Key components of safe fail mechanisms include the ability to detect and isolate failures promptly, mechanisms to mitigate the impact of such failures, and systems to alert users or operators of the issue. This multifaceted approach ensures that when failures occur, the system can handle them in a way that prioritizes safety and minimizes negative consequences.

Testing and evaluating these safe fail mechanisms are essential to confirm their effectiveness and reliability. Through rigorous testing under various scenarios, including those that simulate potential failure modes, organizations can assess whether these mechanisms activate appropriately and perform as expected. This evaluation process is crucial for identifying any weaknesses or areas for improvement in the fail-safe designs, ensuring that the AI system can be trusted to handle operational anomalies safely and efficiently.

#### Sub Practices

1. Design and implement safe fail mechanisms within the AI system to ensure graceful degradation or termination in case of unexpected or hazardous situations.

2. Safe fail mechanisms should include mechanisms to identify and isolate failures, reduce the impact of failures, and provide warnings or alerts to users or operators.

3. Test and evaluate safe fail mechanisms to ensure their effectiveness and reliability.

### Measure 2.6.6. Continuous Safety Risk Monitoring.

Establishing a continuous safety risk monitoring system is paramount for maintaining the AI system's integrity and operational safety. This system should be capable of tracking the AI system's ongoing performance, identifying any deviations from expected behavior that could indicate potential safety issues. By proactively detecting anomalies and safety-related concerns, organizations can address issues before they escalate, ensuring the AI system operates within safe parameters. Implementing such a monitoring system ensures a persistent vigilance over the system's safety, facilitating swift identification and resolution of issues that could compromise safety or performance.

Incorporating real-time monitoring capabilities enhances the system's ability to provide immediate feedback on its safety status, allowing for rapid response to any identified risks. This immediate awareness is crucial for maintaining control over the system's operational safety and ensuring that any potential threats are managed promptly. Additionally, establishing clear escalation procedures for critical safety alerts is essential. It guarantees that significant safety concerns are communicated quickly to the relevant decision-makers, ensuring timely and effective intervention. This structured approach to safety risk management underscores an organization's commitment to maintaining the highest safety standards for its AI systems.

#### Sub Practices

1. Establish a continuous safety risk monitoring system to track the AI system's performance, identify potential anomalies, and detect safety-related issues proactively.

2. Implement real-time monitoring capabilities to provide immediate feedback on the AI system's safety status.

3. Implement clear escalation procedures for critical safety alerts to ensure timely intervention and mitigation.

### Measure 2.6.7. Document Safety Risk Evaluation and Mitigation.

Maintaining a comprehensive record of safety risk evaluations, mitigation strategies, and test results is crucial for ensuring the AI system's ongoing safety and reliability. This documentation provides a detailed account of the safety assessments conducted, the rationale for prioritizing certain risks, and the effectiveness of the mitigation strategies implemented. Such records not only serve as a historical reference but also facilitate the continuous improvement of safety measures by providing insights into what strategies have been successful and where adjustments are needed. Documenting the decision-making process behind safety measures also enhances transparency and accountability, ensuring that safety practices are grounded in rigorous analysis and systematic evaluation.

Sharing this safety documentation with relevant stakeholders, including developers, operators, and risk managers, is essential for promoting a culture of safety and collaboration. By making this information accessible, all parties involved in the AI system's lifecycle are informed about its safety status and the measures in place to mitigate risks. This shared understanding fosters a proactive approach to safety management, where stakeholders can contribute to the ongoing evaluation and enhancement of safety practices. Moreover, this collaborative environment supports informed decision-making, ensuring that safety remains a paramount concern in the development and operation of AI systems.

#### Sub Practices

1. Maintain a comprehensive record of safety risk evaluations, mitigation strategies, and test results.

2. Document the rationale behind risk prioritization, mitigation choices, and safety decision-making processes.

3. Share safety risk documentation with relevant stakeholders, including developers, operators, and risk managers.

### Measure 2.6.8. Continuously Evaluate and Enhance Safety Risk Management.

Continuous evaluation and enhancement of safety risk management practices are vital for ensuring the enduring safety of AI systems. This process involves regularly assessing the effectiveness of current safety measures and identifying potential areas for improvement. As the AI system evolves and new challenges emerge, it's crucial to adapt and refine safety strategies to address these changes effectively. This ongoing evaluation ensures that the safety risk management framework remains robust and responsive to the dynamic nature of AI technologies and operational environments.

Soliciting feedback from a wide range of stakeholders, including developers, operators, and users, is essential for gaining diverse perspectives on the AI system's safety. This feedback can provide valuable insights into the system's performance in real-world settings, highlighting potential safety concerns that may not have been evident during initial assessments. Additionally, staying abreast of emerging safety risks and advancements in safety engineering allows organizations to proactively update their safety measures. By integrating the latest best practices and technologies into their safety risk management framework, organizations can ensure that their AI systems maintain the highest safety standards in the face of evolving threats and technological progress.

#### Sub Practices

1. Regularly evaluate the effectiveness of the AI system's safety risk management practices, identifying areas for improvement and adapting strategies as the system evolves.

2. Gather feedback from stakeholders, including developers, operators, and users, to refine safety risk assessment and mitigation approaches.

3. Stay informed about emerging safety risks and technological advancements in safety engineering to maintain the AI system's safety posture.

### Measure 2.6 Suggested Work Products

* Safety Risk Evaluation Framework Document - A comprehensive document outlining the framework for evaluating safety risks, including identification, assessment, and mitigation processes.
* Safety Risk Register - A detailed log of identified safety risks categorized by severity, likelihood, and potential impact, including their current status and mitigation measures.
* Residual Risk Acceptance Criteria - Documentation defining clear criteria for determining the acceptability of residual risks post-mitigation efforts.
* Safety Metrics and Thresholds Report - A report specifying the quantitative and qualitative metrics used to evaluate the AI system's safety and the thresholds set for each metric.
* Safety Risk Assessment Reports - Periodic reports from regular safety risk assessments throughout the AI system's lifecycle, documenting findings and recommendations.
* Safety Risk Mitigation Plan - A strategic plan detailing prioritized safety risks and the specific mitigation strategies to be implemented, including timelines and responsible parties.
* Safe Fail Mechanisms Design Document - Technical documentation on the design and implementation of safe fail mechanisms within the AI system, including test results confirming their effectiveness.
* Safety Enhancement Review Report - An annual or bi-annual report summarizing the outcomes of the continuous evaluation and enhancement efforts for safety risk management practices, including stakeholder feedback and updates to safety measures.
