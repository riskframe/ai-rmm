[//]: # (COPYRIGHT)
[//]: # (RiskFrame.ai - AI Risk Management and Resilience Framework)
[//]: # (Copyright (C) 2024 RiskFrame.ai https://www.riskframe.ai https://github.com/riskframe/ai-rmm)
[//]: # (SOFTWARE LICENSE)
[//]: # (This file is part of AI-RMM, which is distributed under GNU General Public License V3. See LICENSE.txt to get a full copy.)
    
## Measure 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are involved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.

### Measure 1.3.1. Engage Internal and External Expertise.

#### Sub Practices

1. Regularly involve internal experts who did not directly participate in the AI system's development to provide independent perspectives and assessments.

2. Collaborate with independent assessors or consultants with expertise in AI governance, risk management, and responsible AI practices.

3. Establish a process for selecting and onboarding external assessors, ensuring their competence, impartiality, and adherence to ethical principles.

### Measure 1.3.2. Consult with Domain Experts and Users.

#### Sub Practices

1. Seek input from domain experts with deep knowledge of the AI system's application domain to assess its relevance, effectiveness, and potential impacts.

2. Engage with actual users of the AI system to understand their experiences, feedback, and concerns related to its performance, trustworthiness, and ethical implications.

3. Establish clear channels for feedback and collaboration with users, ensuring their voices are heard and incorporated into the assessment process.

### Measure 1.3.3. Involve AI Actors and Affected Communities.

#### Sub Practices

1. Collaborate with AI actors external to the development team, such as researchers, ethicists, and industry experts, to gain diverse perspectives on the AI system's trustworthiness and potential impacts.

2. Consult with representatives of affected communities who may be particularly vulnerable to the AI system's decisions or outputs.

3. Establish mechanisms for open and respectful engagement with AI actors and affected communities, prioritizing their concerns and perspectives.

### Measure 1.3.4. Tailor Assessment Involvement to Risk Tolerance.

#### Sub Practices

1. Prioritize the involvement of external experts and stakeholders based on the organization's risk tolerance and the perceived severity of AI risks associated with the system.

2. For AI systems with high-risk profiles, engage a wider range of external experts and consult with affected communities more extensively.

3. For AI systems with lower-risk profiles, involve internal experts and selectively consult with domain experts and users.

### Measure 1.3.5. Document Assessment Involvement and Seek Clear Roles.

#### Sub Practices

1. Clearly document the involvement of internal and external experts, domain experts, users, AI actors, and affected communities in the assessment process.

2. Assign clear roles and responsibilities to each participant, ensuring their contributions are effectively coordinated and utilized.

3. Maintain a record of consultations and feedback, including summarized insights and recommendations from each source.

### Measure 1.3.6. Continuously Evaluate and Adapt Assessment Approach.

#### Sub Practices

1. Regularly evaluate the effectiveness of the assessment approach and the level of involvement of external experts and stakeholders.

2. Seek feedback from participants on the assessment process and their perceived contribution to the overall assessment outcomes.

